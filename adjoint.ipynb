{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions representing dz/dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode_torch(t,y):\n",
    "    '''Example differential equation\n",
    "    Args: y is initial value\n",
    "          t is time'''\n",
    "    t = torch.tensor(t)\n",
    "    m = y - 1/2 * torch.exp(t/2) * torch.sin(5*t) + 5*torch.exp(t/2)*torch.cos(5*t)\n",
    "    return m\n",
    "\n",
    "class mini_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mini_net, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        # This is dz/dt (change in hidden state with respect to time)\n",
    "        return(sig(self.W * y))\n",
    "    \n",
    "def sig(x):\n",
    "    return(torch.exp(x) / (torch.exp(x) + 1))\n",
    "\n",
    "class ode_forward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ode_forward, self).__init__()\n",
    "#         self.W = nn.Parameter(torch.randn(6)) # parameters\n",
    "        self.A = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.B = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.C = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.D = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.E = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.F = nn.Parameter(torch.randn(1)) # parameters\n",
    "        \n",
    "    def forward(self,t,y):\n",
    "        '''Example differential equation\n",
    "        Args: y is initial value\n",
    "              t is time'''\n",
    "        t = torch.tensor(t)\n",
    "        m = y - self.A * torch.exp(t/self.B) * torch.sin(self.C*t) +\\\n",
    "                                self.D*torch.exp(t/self.E)*torch.cos(self.F*t)\n",
    "        return m\n",
    "\n",
    "# f = ax + bt\n",
    "class diff_eq(nn.Module):\n",
    "    '''This class represents the 'f' function (change in hidden state with respect to time)'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(diff_eq, self).__init__()\n",
    "        \n",
    "        self.a = nn.Parameter(torch.tensor([3.]))\n",
    "        self.b = nn.Parameter(torch.tensor([1.]))\n",
    "        \n",
    "    def forward(self, t, z):\n",
    "        return(self.a*z + self.b*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk(fun, t, y0):\n",
    "    '''Function performs RK4 algorithm\n",
    "        Args: fun is a function representing the derivative of the state with respect to time\n",
    "              t is list of times to obtain estimates for\n",
    "              y0 is the initial condition of the state\n",
    "        Return: tensor reprsenting state at each time'''\n",
    "    y = [y0]\n",
    "    y_ = y0\n",
    "    for i, t_ in enumerate(t[:-1]):\n",
    "        h = t[1]-t[0]\n",
    "        y_ = rk_step(fun, t=t[i], y=y_, h=h)\n",
    "        y.append(y_)   \n",
    "    return torch.cat(y,dim=0)\n",
    "\n",
    "def rk_step(fun, t, y, h):\n",
    "    '''Function takes a single step in RK4 algorithm\n",
    "        Args: fun is a function representing the derivative of the state with respect to time\n",
    "              t is a two-element tuple representing the initial and final end times\n",
    "              y0 is the initial condition of the state'''\n",
    "    k1 = h * fun(t     , y)\n",
    "    k2 = h * fun(t+.5*h, y+.5*k1)\n",
    "    k3 = h * fun(t+.5*h, y+.5*k2)\n",
    "    k4 = h * fun(t+   h, y+   k3)\n",
    "    \n",
    "    w = torch.tensor([[1/6, 1/3, 1/3, 1/6]]) # weights\n",
    "    y_ = y + torch.sum(w*torch.cat([k1,k2,k3,k4],dim=1), dim=1).view(-1,1)\n",
    "    \n",
    "    return y_\n",
    "\n",
    "def euler(fun, t_span, y0, h, states = None):\n",
    "    '''Function to compute numerical approximation of ODE. \n",
    "    Args: - dy_dt is a function which takes in y,t and represents the derivative\n",
    "          - y0 and t0 represent the initial value and time, respectively\n",
    "          - h is the step size (fixed, in this case)\n",
    "          - n_steps is the number of steps to take'''\n",
    "    n_steps = int((t_span[1]-t_span[0])/h)\n",
    "    y = y0 # set state to initial state\n",
    "    t = t_span[0]\n",
    "    all_y = [y0] # set state to initial state\n",
    "    all_t = [t_span[0]]\n",
    "    for i in range(1, n_steps+1):\n",
    "        if states is None:\n",
    "#             m = fun(t,y,states) # compute derivative at t0\n",
    "            m = fun(t,y)\n",
    "        else:\n",
    "            m = fun(t,y,states[-i], h=h)\n",
    "        y = y + h*m # approximate next value\n",
    "        t = t + h\n",
    "        all_y.append(y) # record values\n",
    "        all_t.append(t)  \n",
    "    return(torch.stack(all_y, dim=0), torch.tensor(all_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aug_dynamics(nn.Module):\n",
    "    '''Class representing augmented dynamics of the system. Forward dynamics are governed by .\n",
    "        To update parameters, we require derivative of loss with respect to the hidden state, \n",
    "        dL/dz (i.e., the adjoint). To compute the adjoint and the derivative of the loss with \n",
    "        respect to the parameters, we use will use the ODE solver with the augmented dynamics.'''\n",
    "    \n",
    "    def __init__(self, model, len_hidden_state):\n",
    "        super(aug_dynamics, self).__init__()\n",
    "        \n",
    "        self.model = model # model to represent the dynamics of hidden state (i.e. dz/dt)\n",
    "        self.len_hidden_state = len_hidden_state\n",
    "\n",
    "    def forward(self, t, aug_state):\n",
    "        '''Forward pass of the dynamics. That is, evaluate the derivatives.\n",
    "            Args: t is time\n",
    "                  aug_state is tensor concatenation [a,z], i.e. adjoint state and hidden state\n",
    "                  fun is function used to evaluate ode_vjp\n",
    "            Returns: derivatives of adjoint and hidden state (respectively) with respect to time\n",
    "                        i.e. torch.concat([da/dt, dz/dt])'''\n",
    "\n",
    "        dt = t[1]-t[0]\n",
    "        a = aug_state[:self.len_hidden_state,:]\n",
    "        z = aug_state[self.len_hidden_state:2*self.len_hidden_state,:].detach().clone().requires_grad_(True)\n",
    "        \n",
    "        dz_dt = self.model(t[0], z) # evaluate function \n",
    "        \n",
    "        self.model.zero_grad() # zero gradients in model\n",
    "        dz_dt.backward(a) # backward step – this updates the gradients in the model parameters\n",
    "        \n",
    "        da_dt = -z.grad\n",
    "        da_theta_dt = -torch.tensor([[p.grad for p in self.model.parameters()]])\n",
    "        \n",
    "        return torch.cat([da_dt, dz_dt.detach(), da_theta_dt], dim=1).transpose(0,1)\n",
    "    \n",
    "def get_loss_and_adjoint(loss_fn, y, y_forward):\n",
    "    '''Function to compute loss and adjoint (initial condition for backward pass)\n",
    "        Args: loss_fn is function to compute loss between ground truth and predicted value\n",
    "              y is ground truth\n",
    "              y_forward are the outputs of the forward pass at each time step\n",
    "        Returns: loss is a scalar\n",
    "                 a is the adjoint at the final time – i.e., the derivative of the loss\n",
    "                     with respect to the final hidden state'''\n",
    "    z1 = y_forward[-1:,:].detach().clone().requires_grad_(True) # get final hidden state\n",
    "    loss = loss_fn(y, z1) # compute loss\n",
    "#     loss = z1\n",
    "    loss.backward() # compute adjoint at t = t_1\n",
    "    a = z1.grad # get initial adjoint state (gradient of loss W.R.T. final hidden state)\n",
    "    return loss, a # return loss and adjoint state\n",
    "\n",
    "def my_backward(func, t, y_forward, a):\n",
    "    '''Custom backward function to update parameters using adjoint method\n",
    "        Args: func is the function representing derivative of hidden state with respect to time\n",
    "              t is a vector representing the function evaluation times for the forward pass\n",
    "              y_forward is a tensor representing the hidden states at each time in the forward pass'''\n",
    "    t = torch.flip(t,dims=(0,)) # flip time and y_forward along time axis\n",
    "    z = torch.flip(y_forward,dims=(0,))\n",
    "    dyn = aug_dynamics(model=func, len_hidden_state = len(a)) # define hidden dynamics function\n",
    "    aug_state = torch.cat([a, z[0:1], # Initialize augmented state\n",
    "                           torch.tensor([[0. for p in func.parameters()]])],dim=1).transpose(0,1) \n",
    "    for i, t_ in enumerate(t[:-1]): # Traverse time steps and hidden states in reverse\n",
    "        h_ = t[i+1] - t[i] # get step size\n",
    "        aug_state[len(a):2*len(a)] = z[i:i+1] # reset hidden state to be ground truth\n",
    "        \n",
    "        # Go to next time step, using ODE solver\n",
    "        aug_state = rk_step(fun=dyn.forward, t = torch.tensor([t[i], t[i+1]]), y=aug_state, h=h_)\n",
    "\n",
    "    # update parameters\n",
    "    for i, p in enumerate(func.parameters()):\n",
    "        p.grad = aug_state[i+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare normal backprop with custom adjoint and torchdiffeq adjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-dimensional hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated parameters (backprop through solver)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1332.2523], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.2314], requires_grad=True))]\n",
      "\n",
      "Updated parameters (custom adjoint)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1332.2555], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.2320], requires_grad=True))]\n",
      "\n",
      "Updated parameters (torchdiffeq adjoint)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1332.2605], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.2326], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "y = torch.randn(1)\n",
    "\n",
    "# Parameters for Runge-Kutta\n",
    "t_span = [1.,2.] # start/end times \n",
    "h=.01 # step size\n",
    "t = torch.tensor(list(np.concatenate([np.arange(*t_span, h), [t_span[-1]]]))) # get evaluation times\n",
    "lr = .3 # set learning rate for optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "f = ode_forward()\n",
    "\n",
    "################ backpropping through solver ##############\n",
    "torch.manual_seed(0)\n",
    "# diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "loss = y_forward[-1]\n",
    "loss = loss_fn(y_forward[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (backprop through solver)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using custom adjoint ##############\n",
    "torch.manual_seed(0)\n",
    "# diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "#### ADJOINT METHOD ####\n",
    "diffeq.zero_grad()\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "loss, a = get_loss_and_adjoint(loss_fn, y, y_forward)\n",
    "my_backward(diffeq, t, y_forward, a)\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (custom adjoint)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using torchdiffeq package ##############\n",
    "from torchdiffeq import odeint_adjoint, odeint\n",
    "method = 'rk4'\n",
    "torch.manual_seed(0)    \n",
    "# diffeq = ode_forward()\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "yhat = odeint_adjoint(func=diffeq, y0=torch.tensor([2.]), t = t, method=method)\n",
    "loss = loss_fn(yhat[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (torchdiffeq adjoint)')\n",
    "print(list(diffeq.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-D hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "y = torch.randn(1)\n",
    "\n",
    "# Parameters for Runge-Kutta\n",
    "t_span = [1.,2.] # start/end times \n",
    "h=.01 # step size\n",
    "t = torch.tensor(list(np.concatenate([np.arange(*t_span, h), [t_span[-1]]]))) # get evaluation times\n",
    "lr = .3 # set learning rate for optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "f = ode_forward()\n",
    "\n",
    "################ backpropping through solver ##############\n",
    "torch.manual_seed(0)\n",
    "diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "#### ADJOINT METHOD ####\n",
    "diffeq.zero_grad()\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "\n",
    "loss = y_forward[-1]\n",
    "loss = loss_fn(y_forward[-1], y)\n",
    "loss.backward()\n",
    "    \n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (backprop through solver)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using custom adjoint ##############\n",
    "torch.manual_seed(0)\n",
    "diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "#### ADJOINT METHOD ####\n",
    "diffeq.zero_grad()\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "\n",
    "# loss, a = get_loss_and_adjoint(None, None, y_forward)\n",
    "loss, a = get_loss_and_adjoint(loss_fn, y, y_forward)\n",
    "my_backward(diffeq, t, y_forward, a)\n",
    "    \n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (custom adjoint)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using torchdiffeq package ##############\n",
    "from torchdiffeq import odeint_adjoint, odeint\n",
    "method = 'rk4'\n",
    "    \n",
    "torch.manual_seed(0)    \n",
    "diffeq = ode_forward()\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "yhat = odeint_adjoint(func=diffeq, y0=torch.tensor([2.]), t = t, method=method)\n",
    "loss = loss_fn(yhat[-1], y)\n",
    "# yhat[-1].backward()\n",
    "loss.backward()\n",
    "    \n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (torchdiffeq adjoint)')\n",
    "print(list(diffeq.named_parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrtl_env",
   "language": "python",
   "name": "mrtl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
