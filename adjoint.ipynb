{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions representing dz/dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = ax + bt\n",
    "class diff_eq(nn.Module):\n",
    "    '''This class represents the 'f' function (change in hidden state with respect to time)'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(diff_eq, self).__init__()\n",
    "        \n",
    "        self.a = nn.Parameter(torch.tensor([3.]))\n",
    "        self.b = nn.Parameter(torch.tensor([1.]))\n",
    "        \n",
    "    def forward(self, t, z):\n",
    "        return(self.a*z + self.b*t)\n",
    "\n",
    "class mini_net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mini_net, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(1))\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        # This is dz/dt (change in hidden state with respect to time)\n",
    "        return(sig(self.W * y))\n",
    "    \n",
    "class small_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim=5):\n",
    "        super(small_net, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(dim,dim))\n",
    "        self.Y = nn.Parameter(torch.randn(dim,dim))\n",
    "        \n",
    "    def forward(self, t, y):\n",
    "        # This is dz/dt (change in hidden state with respect to time)\n",
    "        out = sig(self.W @ y)\n",
    "        return(self.Y @ out)\n",
    "    \n",
    "def sig(x):\n",
    "    return(torch.exp(x) / (torch.exp(x) + 1))\n",
    "\n",
    "class ode_forward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ode_forward, self).__init__()\n",
    "#         self.W = nn.Parameter(torch.randn(6)) # parameters\n",
    "        self.A = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.B = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.C = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.D = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.E = nn.Parameter(torch.randn(1)) # parameters\n",
    "        self.F = nn.Parameter(torch.randn(1)) # parameters\n",
    "        \n",
    "    def forward(self,t,y):\n",
    "        '''Example differential equation\n",
    "        Args: y is initial value\n",
    "              t is time'''\n",
    "        t = torch.tensor(t)\n",
    "        m = y - self.A * torch.exp(t/self.B) * torch.sin(self.C*t) +\\\n",
    "                                self.D*torch.exp(t/self.E)*torch.cos(self.F*t)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rk(fun, t, y0):\n",
    "    '''Function performs RK4 algorithm\n",
    "        Args: fun is a function representing the derivative of the state with respect to time\n",
    "              t is list of times to obtain estimates for\n",
    "              y0 is the initial condition of the state\n",
    "        Return: tensor reprsenting state at each time'''\n",
    "    y = [y0]\n",
    "    y_ = y0\n",
    "    for i, t_ in enumerate(t[:-1]):\n",
    "        h = t[1]-t[0]\n",
    "        y_ = rk_step(fun, t=t[i], y=y_, h=h)\n",
    "        y.append(y_)   \n",
    "    return y\n",
    "\n",
    "def rk_step(fun, t, y, h):\n",
    "    '''Function takes a single step in RK4 algorithm\n",
    "        Args: fun is a function representing the derivative of the state with respect to time\n",
    "              t is a two-element tuple representing the initial and final end times\n",
    "              y0 is the initial condition of the state'''\n",
    "    k1 = h * fun(t     , y)\n",
    "    k2 = h * fun(t+.5*h, y+.5*k1)\n",
    "    k3 = h * fun(t+.5*h, y+.5*k2)\n",
    "    k4 = h * fun(t+   h, y+   k3)\n",
    "    \n",
    "    w = torch.tensor([1/6, 1/3, 1/3, 1/6]) # weights\n",
    "    w = w[(None,)*y.ndimension()] # add singleton dimensions to match shape of ks\n",
    "    y_ = y + torch.sum(w*torch.stack([k1,k2,k3,k4],dim=-1), dim=-1)\n",
    "    \n",
    "    return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjoint method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aug_dynamics(nn.Module):\n",
    "    '''Class representing augmented dynamics of the system. Forward dynamics are governed by .\n",
    "        To update parameters, we require derivative of loss with respect to the hidden state, \n",
    "        dL/dz (i.e., the adjoint). To compute the adjoint and the derivative of the loss with \n",
    "        respect to the parameters, we use will use the ODE solver with the augmented dynamics.'''\n",
    "    \n",
    "    def __init__(self, model, init_hidden_state):\n",
    "        super(aug_dynamics, self).__init__()\n",
    "        \n",
    "        self.model = model # model to represent the dynamics of hidden state (i.e. dz/dt)\n",
    "        self.init_hidden_state = init_hidden_state\n",
    "\n",
    "    def forward(self, t, aug_state):\n",
    "        '''Forward pass of the dynamics. That is, evaluate the derivatives.\n",
    "            Args: t is time\n",
    "                  aug_state is tensor concatenation [a,z], i.e. adjoint state and hidden state\n",
    "                  fun is function used to evaluate ode_vjp\n",
    "            Returns: derivatives of adjoint and hidden state (respectively) with respect to time\n",
    "                        i.e. torch.concat([da/dt, dz/dt])'''\n",
    "        dt = t[1]-t[0]\n",
    "        a,z,_ = unpack_aug_state(aug_state, self.init_hidden_state, list(self.model.parameters()))\n",
    "        z = z.detach().requires_grad_()\n",
    "        \n",
    "        dz_dt = self.model(t[0], z) # evaluate function \n",
    "        \n",
    "        self.model.zero_grad() # zero gradients in model\n",
    "        dz_dt.backward(a) # backward step – this updates the gradients in the model parameters\n",
    "        \n",
    "        da_dt = -z.grad        \n",
    "        da_theta_dt = [-p.grad for p in self.model.parameters()]\n",
    "        aug_state_grad = pack_aug_state(da_dt, dz_dt.detach(), da_theta_dt)\n",
    "        \n",
    "        return aug_state_grad\n",
    "    \n",
    "def get_loss_and_adjoint(loss_fn, y, y_forward):\n",
    "    '''Function to compute loss and adjoint (initial condition for backward pass)\n",
    "        Args: loss_fn is function to compute loss between ground truth and predicted value\n",
    "              y is ground truth\n",
    "              y_forward are the outputs of the forward pass at each time step\n",
    "        Returns: loss is a scalar\n",
    "                 a is the adjoint at the final time – i.e., the derivative of the loss\n",
    "                     with respect to the final hidden state'''\n",
    "    z1 = y_forward[-1].detach().clone().requires_grad_(True) # get final hidden state\n",
    "    loss = loss_fn(y, z1) # compute loss\n",
    "#     loss = z1\n",
    "    loss.backward() # compute adjoint at t = t_1\n",
    "    a = z1.grad # get initial adjoint state (gradient of loss W.R.T. final hidden state)\n",
    "    return loss, a # return loss and adjoint state\n",
    "\n",
    "def my_backward(func, t, y_forward, a):\n",
    "    '''Custom backward function to update parameters using adjoint method\n",
    "        Args: func is the function representing derivative of hidden state with respect to time\n",
    "              t is a vector representing the function evaluation times for the forward pass\n",
    "              y_forward is a tensor representing the hidden states at each time in the forward pass'''\n",
    "    t = torch.flip(t,dims=(0,)) # flip time and y_forward along time axis\n",
    "    z = y_forward[::-1]\n",
    "    dyn = aug_dynamics(model=func, init_hidden_state = z[0].detach()) # define hidden dynamics function\n",
    "    \n",
    "    theta_grad = [torch.zeros_like(p) for p in list(func.parameters())] # get model parameters\n",
    "    aug_state = pack_aug_state(a,z[0],theta_grad)\n",
    "\n",
    "    for i, t_ in enumerate(t[:-1]): # Traverse time steps and hidden states in reverse\n",
    "        h_ = t[i+1] - t[i] # get step size\n",
    "        aug_state[a.numel():2*a.numel()] = z[i].view(-1) # reset hidden state to be ground truth\n",
    "        \n",
    "        # Go to next time step, using ODE solver\n",
    "        aug_state = rk_step(fun=dyn.forward, t = torch.tensor([t[i], t[i+1]]), y=aug_state, h=h_)\n",
    "\n",
    "    # update parameters\n",
    "    _,_,theta_grad = unpack_aug_state(aug_state, z[0], theta_grad)\n",
    "    for i, p in enumerate(func.parameters()):\n",
    "        p.grad = theta_grad[i]\n",
    "\n",
    "def pack_aug_state(a,z,theta_grad):\n",
    "    '''Function packs the adjoint state, hidden state, and parameter gradients into vector for ODE solver\n",
    "        Args: a is adjoint state\n",
    "              z is hidden state\n",
    "              theta is list of tensors, where each tensor corresponds to single variable\n",
    "        Returns: augmented state is a vector with adjoint state, hidden state, and model parameters'''\n",
    "    return torch.cat([a.view(-1), z.view(-1), torch.cat([p.view(-1) for p in theta_grad])]).detach()\n",
    "\n",
    "def unpack_aug_state(aug_state, z_dummy, parameter_dummy):\n",
    "    '''Function unpacks the augmented state into the adjoint state, hidden state, and parameters\n",
    "        Args: aug_state is a vector representing the augmented state\n",
    "              hidden_state_shape is torch.Size representing shape of the hidden state\n",
    "              parameter_shape is list-of(torch.Size) representing size of each parameter in model\n",
    "        Returns: a is adjoint\n",
    "                 z is hidden state\n",
    "                 theta'''\n",
    "    a = aug_state[:z_dummy.numel()].view(z_dummy.shape)\n",
    "    z = aug_state[z_dummy.numel():2*z_dummy.numel()].view(z_dummy.shape)\n",
    "    packed_params = aug_state[2*z_dummy.numel():]\n",
    "    theta_grad = []\n",
    "    idx = 0\n",
    "    for p in parameter_dummy:\n",
    "        theta_grad.append(packed_params[idx:idx+p.numel()].view(p.shape))\n",
    "        idx += p.numel()\n",
    "        \n",
    "    return a,z,theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare normal backprop with custom adjoint and torchdiffeq adjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-dimensional hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated parameters (backprop through solver)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1336.1219], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.9217], requires_grad=True))]\n",
      "\n",
      "Updated parameters (custom adjoint)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1336.1255], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.9224], requires_grad=True))]\n",
      "\n",
      "Updated parameters (torchdiffeq adjoint)\n",
      "[('a', Parameter containing:\n",
      "tensor([-1336.1305], requires_grad=True)), ('b', Parameter containing:\n",
      "tensor([-237.9231], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "y = torch.randn(1)\n",
    "\n",
    "# Parameters for Runge-Kutta\n",
    "t_span = [1.,2.] # start/end times \n",
    "h=.01 # step size\n",
    "t = torch.tensor(list(np.concatenate([np.arange(*t_span, h), [t_span[-1]]]))) # get evaluation times\n",
    "lr = .3 # set learning rate for optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "f = ode_forward()\n",
    "\n",
    "################ backpropping through solver ##############\n",
    "torch.manual_seed(0)\n",
    "# diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "loss = y_forward[-1]\n",
    "loss = loss_fn(y_forward[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (backprop through solver)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using custom adjoint ##############\n",
    "torch.manual_seed(0)\n",
    "# diffeq = ode_forward() # function representing derivative of hidden state with respect to parameters\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "#### ADJOINT METHOD ####\n",
    "diffeq.zero_grad()\n",
    "y_forward = solver(fun=diffeq, t=t, y0=torch.tensor([[2.]])) # Forward pass\n",
    "loss, a = get_loss_and_adjoint(loss_fn, y, y_forward)\n",
    "my_backward(diffeq, t, y_forward, a)\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (custom adjoint)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using torchdiffeq package ##############\n",
    "from torchdiffeq import odeint_adjoint, odeint\n",
    "method = 'rk4'\n",
    "torch.manual_seed(0)    \n",
    "# diffeq = ode_forward()\n",
    "# diffeq = mini_net()\n",
    "diffeq = diff_eq()\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "yhat = odeint_adjoint(func=diffeq, y0=torch.tensor([2.]), t = t, method=method)\n",
    "loss = loss_fn(yhat[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (torchdiffeq adjoint)')\n",
    "print(list(diffeq.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-D hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated parameters (backprop through solver)\n",
      "[('W', Parameter containing:\n",
      "tensor([[-1.1335, -1.2001, -0.2830, -0.4458,  0.8574],\n",
      "        [ 0.7190, -0.1226, -2.0350,  0.3863, -0.2137],\n",
      "        [ 1.3352, -0.4272, -0.2711,  0.6907, -0.0038],\n",
      "        [-0.1010, -0.5419,  1.2960,  2.0265,  0.0359],\n",
      "        [ 0.6098, -0.4651, -0.8746, -2.3297, -0.0920]], requires_grad=True)), ('Y', Parameter containing:\n",
      "tensor([[-0.7411,  0.5684,  0.5148,  0.0708, -0.6752],\n",
      "        [ 0.9338,  0.4604, -1.5833, -0.7748,  1.0989],\n",
      "        [ 1.2833, -1.5199,  1.2361, -1.7298,  0.3181],\n",
      "        [ 1.5101,  2.0870, -0.2958, -0.3247, -1.0645],\n",
      "        [ 1.1116, -0.1611,  0.1360, -0.3055, -0.7695]], requires_grad=True))]\n",
      "\n",
      "Updated parameters (custom adjoint)\n",
      "[('W', Parameter containing:\n",
      "tensor([[-1.1335, -1.2001, -0.2830, -0.4458,  0.8574],\n",
      "        [ 0.7190, -0.1226, -2.0350,  0.3863, -0.2137],\n",
      "        [ 1.3352, -0.4272, -0.2711,  0.6907, -0.0038],\n",
      "        [-0.1010, -0.5419,  1.2960,  2.0265,  0.0359],\n",
      "        [ 0.6098, -0.4651, -0.8746, -2.3297, -0.0920]], requires_grad=True)), ('Y', Parameter containing:\n",
      "tensor([[-0.7411,  0.5684,  0.5148,  0.0708, -0.6752],\n",
      "        [ 0.9338,  0.4604, -1.5833, -0.7748,  1.0989],\n",
      "        [ 1.2833, -1.5199,  1.2361, -1.7298,  0.3181],\n",
      "        [ 1.5101,  2.0870, -0.2958, -0.3247, -1.0645],\n",
      "        [ 1.1116, -0.1611,  0.1360, -0.3055, -0.7695]], requires_grad=True))]\n",
      "\n",
      "Updated parameters (torchdiffeq adjoint)\n",
      "[('W', Parameter containing:\n",
      "tensor([[-1.1335, -1.2001, -0.2830, -0.4458,  0.8574],\n",
      "        [ 0.7190, -0.1226, -2.0350,  0.3863, -0.2137],\n",
      "        [ 1.3352, -0.4272, -0.2711,  0.6907, -0.0038],\n",
      "        [-0.1010, -0.5419,  1.2960,  2.0265,  0.0359],\n",
      "        [ 0.6098, -0.4651, -0.8746, -2.3297, -0.0920]], requires_grad=True)), ('Y', Parameter containing:\n",
      "tensor([[-0.7411,  0.5684,  0.5148,  0.0708, -0.6752],\n",
      "        [ 0.9338,  0.4604, -1.5833, -0.7748,  1.0989],\n",
      "        [ 1.2833, -1.5199,  1.2361, -1.7298,  0.3181],\n",
      "        [ 1.5101,  2.0870, -0.2958, -0.3247, -1.0645],\n",
      "        [ 1.1116, -0.1611,  0.1360, -0.3055, -0.7695]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "y = torch.randn(5)\n",
    "y0 = torch.randn(5)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Parameters for Runge-Kutta\n",
    "t_span = [1.,2.] # start/end times \n",
    "h=.01 # step size\n",
    "t = torch.tensor(list(np.concatenate([np.arange(*t_span, h), [t_span[-1]]]))) # get evaluation times\n",
    "lr = 1 # set learning rate for optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "################ backpropping through solver ##############\n",
    "torch.manual_seed(0)\n",
    "diffeq = small_net() # function representing derivative of hidden state with respect to parameters\n",
    "# print('Original parameters')\n",
    "# print(list(diffeq.named_parameters()))\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "y_forward = solver(fun=diffeq, t=t, y0=y0) # Forward pass\n",
    "loss = loss_fn(y_forward[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (backprop through solver)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using custom adjoint ##############\n",
    "torch.manual_seed(0)\n",
    "diffeq = small_net()\n",
    "solver = rk\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "#### ADJOINT METHOD ####\n",
    "diffeq.zero_grad()\n",
    "y_forward = solver(fun=diffeq, t=t, y0=y0) # Forward pass\n",
    "loss, a = get_loss_and_adjoint(loss_fn, y, y_forward)\n",
    "my_backward(diffeq, t, y_forward, a)\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (custom adjoint)')\n",
    "print(list(diffeq.named_parameters()))\n",
    "\n",
    "\n",
    "################ using torchdiffeq package ##############\n",
    "from torchdiffeq import odeint_adjoint, odeint\n",
    "method = 'rk4'\n",
    "torch.manual_seed(0)    \n",
    "diffeq = small_net()\n",
    "optimizer = optim.SGD(diffeq.parameters(), lr=lr)\n",
    "\n",
    "yhat = odeint_adjoint(func=diffeq, y0=y0, t = t, method=method)\n",
    "loss = loss_fn(yhat[-1], y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print('\\nUpdated parameters (torchdiffeq adjoint)')\n",
    "print(list(diffeq.named_parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrtl_env",
   "language": "python",
   "name": "mrtl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
